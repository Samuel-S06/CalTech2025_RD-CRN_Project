{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# JAX and TF NCA Model Comparison with Pretrained Weights"
      ],
      "metadata": {
        "id": "9O158fNar_cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### This notebook validates JAX NCA Model with TF NCA Model using pretrained weights from Mordvintsev's NCA experiments.\n",
        "\n",
        "> The aim of this notebook is to inspire the experimentation of JAX NCA Model on target images"
      ],
      "metadata": {
        "id": "ukQeiYDwr_Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installations**"
      ],
      "metadata": {
        "id": "mijCnDjJDttQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install jax\n",
        "# !pip3 install jaxlib\n",
        "# !pip3 install tensorflow\n",
        "# !pip3 install numpy\n"
      ],
      "metadata": {
        "id": "GDQp78vnEHFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "gTkBqHRMEB0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import lax\n",
        "import time\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "VLIoWSkcEFqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tensorflow/Keras Implementation of NCA Model**\n"
      ],
      "metadata": {
        "id": "se25F_aEu3jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_living_mask_tf(x):\n",
        "  alpha = x[:, :, :, 3:4]\n",
        "  return tf.nn.max_pool2d(alpha, 3, [1, 1, 1, 1], 'SAME') > 0.1\n",
        "\n",
        "class CAModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, channel_n, fire_rate):\n",
        "    super().__init__()\n",
        "    self.channel_n = channel_n\n",
        "    self.fire_rate = fire_rate\n",
        "\n",
        "    self.dmodel = tf.keras.Sequential([\n",
        "          Conv2D(128, 1, activation=tf.nn.relu),\n",
        "          Conv2D(self.channel_n, 1, activation=None,\n",
        "              kernel_initializer=tf.zeros_initializer())\n",
        "    ])\n",
        "\n",
        "    self(tf.zeros([1, 3, 3, channel_n]))  # dummy call to build the model\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def perceive(self, x, angle=0.0):\n",
        "    identify = np.float32([0, 1, 0])\n",
        "    identify = np.outer(identify, identify)\n",
        "    dx = np.outer([1, 2, 1], [-1, 0, 1]) / 8.0  # Sobel filter\n",
        "    dy = dx.T\n",
        "    c, s = tf.cos(angle), tf.sin(angle)\n",
        "    kernel = tf.stack([identify, c*dx-s*dy, s*dx+c*dy], -1)[:, :, None, :]\n",
        "    kernel = tf.repeat(kernel, self.channel_n, 2)\n",
        "    y = tf.nn.depthwise_conv2d(x, kernel, [1, 1, 1, 1], 'SAME')\n",
        "    return y\n",
        "\n",
        "  @tf.function\n",
        "  def call(self, x, fire_rate=None, angle=0.0, step_size=1.0):\n",
        "    pre_life_mask = get_living_mask_tf(x)\n",
        "\n",
        "    y = self.perceive(x, angle)\n",
        "    dx = self.dmodel(y)*step_size\n",
        "    if fire_rate is None:\n",
        "      fire_rate = self.fire_rate\n",
        "    update_mask = tf.random.uniform(tf.shape(x[:, :, :, :1])) <= fire_rate\n",
        "    x += dx * tf.cast(update_mask, tf.float32)\n",
        "\n",
        "    post_life_mask = get_living_mask_tf(x)\n",
        "    life_mask = pre_life_mask & post_life_mask\n",
        "    return x * tf.cast(life_mask, tf.float32)"
      ],
      "metadata": {
        "id": "xSHJr4n3u88x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-trained Model Initialization**"
      ],
      "metadata": {
        "id": "ztuEHfThKjHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O models.zip 'https://github.com/google-research/self-organising-systems/blob/master/assets/growing_ca/models.zip?raw=true'\n",
        "!unzip -oq models.zip\n",
        "\n",
        "EMOJI = 'ðŸ¦ŽðŸ˜€ðŸ’¥ðŸ‘ðŸ ðŸ¦‹ðŸžðŸ•¸ðŸ¥¨ðŸŽ„'\n",
        "\n",
        "def get_model(emoji='ðŸ¦‹', fire_rate=0.5, use_pool=1, damage_n=3, run=0,\n",
        "              prefix='models/', output='model'):\n",
        "  path = prefix\n",
        "  assert fire_rate in [0.5, 1.0]\n",
        "  if fire_rate==0.5:\n",
        "    path += 'use_sample_pool_%d damage_n_%d '%(use_pool, damage_n)\n",
        "  elif fire_rate==1.0:\n",
        "    path += 'fire_rate_1.0 '\n",
        "  code = hex(ord(emoji))[2:].upper()\n",
        "  path += 'target_emoji_%s run_index_%d/08000'%(code, run)\n",
        "  assert output in ['model', 'json']\n",
        "  if output == 'model':\n",
        "    ca = CAModel(channel_n=16, fire_rate=fire_rate)\n",
        "    ca.load_weights(path)\n",
        "    return ca\n",
        "  elif output == 'json':\n",
        "    return open(path+'.json', 'r').read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWX6i9ylKjh5",
        "outputId": "8af017eb-8888-4a50-812b-770175c0c69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-03 20:14:19--  https://github.com/google-research/self-organising-systems/blob/master/assets/growing_ca/models.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/google-research/self-organising-systems/raw/refs/heads/master/assets/growing_ca/models.zip [following]\n",
            "--2025-07-03 20:14:19--  https://github.com/google-research/self-organising-systems/raw/refs/heads/master/assets/growing_ca/models.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/google-research/self-organising-systems/refs/heads/master/assets/growing_ca/models.zip [following]\n",
            "--2025-07-03 20:14:20--  https://raw.githubusercontent.com/google-research/self-organising-systems/refs/heads/master/assets/growing_ca/models.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8395316 (8.0M) [application/zip]\n",
            "Saving to: â€˜models.zipâ€™\n",
            "\n",
            "models.zip          100%[===================>]   8.01M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-07-03 20:14:21 (156 MB/s) - â€˜models.zipâ€™ saved [8395316/8395316]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Keras 3 compatability\n",
        "!pip3 install \"keras<3\" \"tensorflow<2.16\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fLFi9cnm3uPI",
        "outputId": "981a8eff-691b-4f9c-cc92-daabb24d224d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras<3\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tensorflow<2.16\n",
            "  Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.16)\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow<2.16)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow<2.16)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<2.16)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.73.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow<2.16)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow<2.16)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.16) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16) (3.3.1)\n",
            "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.1 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 numpy-1.26.4 protobuf-4.25.8 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "keras",
                  "ml_dtypes",
                  "numpy",
                  "tensorflow",
                  "wrapt"
                ]
              },
              "id": "2061e39166fb411183f47ef7721ec5b0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = get_model(emoji='ðŸ¦Ž')\n",
        "\n",
        "# Prints weights and biases of pretrained_model\n",
        "for layer in pretrained_model.layers:\n",
        "    weights = layer.get_weights()\n",
        "    print(layer.name, [w.shape for w in weights])\n",
        "\n",
        "params = pretrained_model.layers[0].get_weights()\n",
        "w1 = params[0]\n",
        "b1 = params[1]\n",
        "w2 = params[2]\n",
        "b2 = params[3]\n",
        "\n",
        "np.save('w1.npy', w1)\n",
        "np.save('b1.npy', b1)\n",
        "np.save('w2.npy', w2)\n",
        "np.save('b2.npy', b2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "-EbIBd-tKknV",
        "outputId": "6cb03b10-3c36-4a35-9e00-27690aaf8839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File format not supported: filepath=models/use_sample_pool_1 damage_n_3 target_emoji_1F98E run_index_0/08000. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-1671151534.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memoji\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ðŸ¦Ž'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Prints weights and biases of pretrained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2948030390.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(emoji, fire_rate, use_pool, damage_n, run, prefix, output)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCAModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfire_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfire_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marguments_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0marguments_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# Get original error message and append information to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(model, filepath, skip_mismatch, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# If file exists and should not be overwritten.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mexists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mexists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File format not supported: filepath=models/use_sample_pool_1 damage_n_3 target_emoji_1F98E run_index_0/08000. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **JAX Implementation of CA Model**"
      ],
      "metadata": {
        "id": "sv5W3PSKMizE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
        "\n",
        "# CA Model and Utilities\n",
        "\n",
        "def get_living_mask_jax(x):\n",
        "  alpha = x[:, :, :, 3:4]\n",
        "  return lax.reduce_window(\n",
        "        alpha,\n",
        "        -jnp.inf,\n",
        "        lax.max,\n",
        "        window_dimensions=(1, 3, 3, 1),\n",
        "        window_strides=(1, 1, 1, 1),\n",
        "        padding='SAME'\n",
        "    ) > 0.1\n",
        "\n",
        "# Load weights and biases\n",
        "w1_jax = jnp.array(np.load('w1.npy'))\n",
        "b1_jax = jnp.array(np.load('b1.npy'))\n",
        "w2_jax = jnp.array(np.load('w2.npy'))\n",
        "b2_jax = jnp.array(np.load('b2.npy'))\n",
        "\n",
        "class CAModelJAX:\n",
        "  # Modified version of __init__ to take in custom weights and biases\n",
        "  def __init__(self, channel_n, fire_rate):\n",
        "    self.channel_n = channel_n\n",
        "    self.fire_rate = fire_rate\n",
        "\n",
        "  # Depthwise Convolution\n",
        "  def perceive(self, x, angle):\n",
        "    identify = jnp.float32([0,1,0])\n",
        "    identify = jnp.outer(identify, identify)\n",
        "    dx = jnp.outer(jnp.array([1,2,1]),jnp.array([-1,0,1])) / 8.0\n",
        "    dy = dx.T\n",
        "    c, s = jnp.cos(angle), jnp.sin(angle)\n",
        "    base_filters = jnp.stack([identify, c*dx-s*dy, s*dx+c*dy])\n",
        "    kernel = jnp.zeros((3,3,1,self.channel_n*3))\n",
        "    for i in range(self.channel_n):\n",
        "      for j in range(3):\n",
        "        kernel = kernel.at[:,:,0,i*3+j].set(base_filters[j])\n",
        "    y = lax.conv_general_dilated(\n",
        "          x, # shape: [1,3,3,16]\n",
        "          kernel, # shape: [3,3,1,48]\n",
        "          window_strides=(1, 1),\n",
        "          padding=\"SAME\",\n",
        "          dimension_numbers=(\"NHWC\", \"HWIO\", \"NHWC\"),\n",
        "          feature_group_count=self.channel_n  # depthwise conv here\n",
        "          )\n",
        "    return y\n",
        "\n",
        "  # Dense Layer Convolutions\n",
        "  def __call__(self, key, x, angle=0.0, fire_rate=None, step_size=1.0): # Input x: (1,3,3,16)\n",
        "    pre_life_mask = get_living_mask_jax(x)\n",
        "\n",
        "    y = self.perceive(x, angle)  # output channels = channel_n * 3 (e.g., 48)\n",
        "    y = lax.conv_general_dilated(y, w1_jax,\n",
        "                                window_strides=(1,1),\n",
        "                                padding='SAME',\n",
        "                                dimension_numbers=(\"NHWC\", \"HWIO\", \"NHWC\")) + b1_jax  # Normal conv, output channels=128\n",
        "    y = jax.nn.relu(y)\n",
        "    y = lax.conv_general_dilated(y, w2_jax,\n",
        "                                window_strides=(1,1),\n",
        "                                padding='SAME',\n",
        "                                dimension_numbers=(\"NHWC\", \"HWIO\", \"NHWC\")) + b2_jax\n",
        "    y *= step_size\n",
        "    if fire_rate is None:\n",
        "      fire_rate = self.fire_rate\n",
        "    update_mask = jax.random.uniform(key, x[:,:,:,:1].shape) <= fire_rate\n",
        "    x += y * jnp.astype(update_mask, jnp.float32)\n",
        "    post_life_mask = get_living_mask_jax(x)\n",
        "    life_mask = pre_life_mask & post_life_mask\n",
        "    return x * jnp.astype(life_mask, jnp.float32)"
      ],
      "metadata": {
        "id": "SHARPbXQMSYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing Output**"
      ],
      "metadata": {
        "id": "jZsLumyavPIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channel_n=16\n",
        "fire_rate = 0.5\n",
        "\n",
        "def make_seed_jax(size, n=1):\n",
        "  x = np.zeros([n, size, size, channel_n], np.float32)\n",
        "  x[:, size//2, size//2, 3:] = 1.0\n",
        "  return jnp.array(x)\n",
        "\n",
        "def make_seed_tf(size, n=1):\n",
        "  x = np.zeros([n, size, size, channel_n], np.float32)\n",
        "  x[:, size//2, size//2, 3:] = 1.0\n",
        "  return x"
      ],
      "metadata": {
        "id": "2L53bTGDwWJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed = make_seed_jax(96)\n",
        "plt.imshow(seed[0,:,:,:4])"
      ],
      "metadata": {
        "id": "FQXcxHSl1weK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_jax = CAModelJAX(channel_n, fire_rate)\n",
        "x = seed\n",
        "steps = 250\n",
        "\n",
        "start_key = jax.random.PRNGKey(0)\n",
        "for i in range(steps):\n",
        "  key, subkey = jax.random.split(start_key, num=2)\n",
        "  x = model_jax(subkey, x)\n",
        "  start_key = key # Update the key for the next iteration\n",
        "\n",
        "plt.imshow(x[0,:,:,:4])"
      ],
      "metadata": {
        "id": "iK5okT0j3pSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed2 = make_seed_tf(96)\n",
        "plt.imshow(seed2[0,:,:,:4])"
      ],
      "metadata": {
        "id": "f2xXoW4V8cVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = seed2\n",
        "\n",
        "for i in range(steps):\n",
        "  y = pretrained_model(y)\n",
        "\n",
        "plt.imshow(y[0,:,:,:4])\n"
      ],
      "metadata": {
        "id": "UqgkLN3F9NaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ol_96i3N--V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}